import gzip
import re
import shutil
import subprocess
import tempfile
import uuid
import zipfile
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Optional, Tuple

from haddock import log
from haddock.libs.libsubprocess import CNSJob

PROXYINFO_CMD = "/opt/diracos/bin/dirac-proxy-info"
SUBMIT_CMD = "/opt/diracos/bin/dirac-wms-job-submit"
STATUS_CMD = "/opt/diracos/bin/dirac-wms-job-status"
GETOUTPUT_CMD = "/opt/diracos/bin/dirac-wms-job-get-output"


def ping_dirac() -> bool:
    """Ping the Dirac caserver to check if it's reachable."""
    result = subprocess.run([PROXYINFO_CMD], shell=True, capture_output=True)
    return result.returncode == 0


class JobStatus(Enum):
    WAITING = "Waiting"
    RUNNING = "Running"
    UNKNOWN = "Unknown"
    DONE = "Done"
    MATCHED = "Matched"
    COMPLETING = "Completing"
    FAILED = "Failed"

    @classmethod
    def from_string(cls, value):
        """Convert string to JobStatus enum."""
        value = value.strip().lower()
        for status in cls:
            if status.value.lower() == value:
                return status
        return cls.UNKNOWN


class GridJob:
    def __init__(self, cnsjob: CNSJob) -> None:
        # Unique name for the job
        self.name = str(uuid.uuid4())
        # Create a temporary directory for the job
        self.loc = Path(tempfile.mkdtemp(prefix="haddock_grid_"))
        # working directory where the GridJob was created, this is important
        #  so we know where to put the files coming from the grid
        self.wd = Path.cwd()
        self.input_f = self.loc / "input.inp"
        # `cns_out_f` is the expected `.out` file that will be generated by this execution
        self.cns_out_f = cnsjob.output_file
        # `expected_outputs` is what will be produced by this `.inp` file
        self.expected_outputs = []
        # `id` is given by DIRAC
        self.id = None
        # Site where the job is running
        self.site = None
        # Output and error files generated by the job.sh execution
        self.stdout_f = None
        self.stderr_f = None
        # Script that will be executed in the grid
        self.job_script = self.loc / "job.sh"
        # JDL file that describes the job to DIRAC
        self.jdl = self.loc / "job.jdl"
        # Internal status of the job
        self.status = JobStatus.UNKNOWN
        # List of files to be included in the payload
        self.payload_fnames = []

        # NOTE: `cnsjob.input_file` can be a string or a path, handle the polymorfism here
        if isinstance(cnsjob.input_file, Path):
            self.input_f = cnsjob.input_file

        elif isinstance(cnsjob.input_file, str):
            with open(self.input_f, "w") as f:
                f.write(cnsjob.input_file)

        # CREATE JOB FILE
        self.create_job_script()

        # CREATE PAYLOAD
        self.prepare_payload(
            cns_exec_path=Path(cnsjob.cns_exec),
            cns_script_path=Path(cnsjob.envvars["MODULE"]),
            toppar_path=Path(cnsjob.envvars["TOPPAR"]),
        )

        # CREATE THE JDL
        self.create_jdl()

    def prepare_payload(
        self, cns_exec_path: Path, cns_script_path: Path, toppar_path: Path
    ):
        """placeholder"""
        # CRAWL THE INPUT FILE AND REPLICATE THE FILE STRUCTURE
        self.process_input_f()

        #  CNS SCRIPTS
        for f in cns_script_path.glob("*"):
            self.payload_fnames.append(Path(f))

        # TOPPAR (including subdirectories)
        for f in toppar_path.rglob("*"):
            if f.is_file():  # Only add files, not directories
                self.payload_fnames.append(f)

        # CREATE PAYLOAD
        with zipfile.ZipFile(f"{self.loc}/payload.zip", "w") as z:
            z.write(self.input_f, arcname="input.inp")
            z.write(cns_exec_path, arcname="cns")
            for f in set(self.payload_fnames):
                # Preserve the relative path structure from toppar_path
                if f.is_relative_to(toppar_path):
                    relative_path = f.relative_to(toppar_path)
                    z.write(f, arcname=str(relative_path))
                else:
                    z.write(f, arcname=Path(f).name)

    def create_job_script(self):
        """placeholder"""
        instructions = """#!/bin/bash
export MODULE=./
export TOPPAR=./
unzip payload.zip
./cns < input.inp > cns.log
"""
        # CREATE JOB SCRIPT
        with open(self.job_script, "w") as f:
            f.write(instructions)

    @staticmethod
    def _process_line(line: str) -> Tuple[str, Optional[str]]:

        # https://regex101.com/r/dYqlZP/1
        VAR_PATTERN = r"\(\$\s*[^=]*=(?!.?\$)(.*)\)"
        AT_PATTERN = r"@@(?!\$)(.*)"

        match_var = re.findall(VAR_PATTERN, line)
        match_at = re.findall(AT_PATTERN, line)

        # NOTE: In CNS it cannot match both
        if match_at:
            item = match_at[0].strip('"').strip("'")
        elif match_var:
            item = match_var[0].strip('"').strip("'")
        else:
            # no match
            return line, None

        if Path(item).exists():
            # This is a path
            return line.replace(item, Path(item).name), item
        else:
            # This is not a path
            return line, None

    @staticmethod
    def _find_output(line) -> Optional[str]:
        # Pattern to match $output_* variables assigned to a filename
        pattern = r'\$output_\w+\s*=\s*"([^"]+)"|\$output_\w+\s*=\s*([^\s;]+)'
        match = re.search(pattern, line)
        if match:
            # Check which group captured the filename
            filename = match.group(1) if match.group(1) else match.group(2)
            return filename
        return None

    def process_input_f(self):
        """Read the `.inp` file, edit the paths and copy the files to the `loc`"""
        original_paths = []
        output_filename = None

        # Read the file first
        with open(self.input_f, "r") as f:
            lines = f.readlines()

        # Write the modified lines back
        with open(self.input_f, "w") as f:
            for line in lines:
                output = self._find_output(line)
                if output:
                    self.expected_outputs.append(output)

                new_line, found_fname = self._process_line(line)

                if found_fname:
                    original_paths.append(Path(found_fname))

                f.write(new_line)

        for src_path in original_paths:
            dst_path = self.loc / src_path.name
            shutil.copy(src_path, dst_path)
            self.payload_fnames.append(dst_path)

        return original_paths, output_filename

    def submit(self) -> None:
        """placeholder"""
        result = subprocess.run(
            [SUBMIT_CMD, f"{self.loc}/job.jdl"],
            shell=False,
            capture_output=True,
            text=True,
            cwd=self.loc,
        )

        self.id = int(result.stdout.split()[-1])
        self.update_status()

    def retrieve_output(self):
        """placeholder"""
        # TODO: Add error handling
        subprocess.run(
            [GETOUTPUT_CMD, str(self.id)],
            shell=False,
            capture_output=True,
            text=True,
            cwd=self.loc,
        )

        # NOTE: This is the output of the `job.sh`
        self.stdout_f = Path(f"{self.loc}/{self.id}/job.out")
        self.stderr_f = Path(f"{self.loc}/{self.id}/job.err")

        # Copy the output to the expected location
        for output_f in self.expected_outputs:
            src = Path(f"{self.loc}/{self.id}/{output_f}")
            dst = Path(self.wd / f"{output_f}")
            shutil.copy(src, dst)

        src = Path(f"{self.loc}/{self.id}/cns.log")
        with open(src, "rb") as f_in, gzip.open(f"{self.cns_out_f}.gz", "wb") as f_out:
            f_out.writelines(f_in)

        self.clean()

    def update_status(self):
        """placeholder"""
        result = subprocess.run(
            [STATUS_CMD, str(self.id)], shell=False, capture_output=True, text=True
        )

        output_dict = self.parse_output(result.stdout)

        self.id = output_dict["JobID"]
        self.status = JobStatus.from_string(output_dict["Status"])
        self.site = output_dict.get("Site", "Unknown")

    def create_jdl(self):
        """placeholder"""
        output_sandbox = ["job.out", "job.err", "cns.log"]
        output_sandbox.extend(self.expected_outputs)
        output_sandbox_str = ", ".join(f'"{fname}"' for fname in output_sandbox)
        jdl_lines = [
            f'JobName = "{self.name}";',
            'Executable = "job.sh";',
            'Arguments = "";',
            'StdOutput = "job.out";',
            'StdError = "job.err";',
            'JobType = "WeNMR-DEV";',
            'Site = "EGI.SARA.nl";',
            'InputSandbox = {"job.sh", "payload.zip"};',
            "OutputSandbox = {" + f"{output_sandbox_str}" + "};",
        ]

        jdl_string = "[\n    " + "\n    ".join(jdl_lines) + "\n]\n"

        with open(self.jdl, "w") as f:
            f.write(jdl_string)

    def clean(self):
        """placeholder"""
        shutil.rmtree(self.loc)

    @staticmethod
    def parse_output(output_str: str) -> dict:
        """placeholder"""
        items = output_str.replace(";", "")
        status_dict = {}
        for item in items.split(" "):
            if "=" in item:
                key, value = item.split("=", 1)
                status_dict[key.strip()] = value.strip()
        return status_dict

    def __repr__(self) -> str:
        return f"ID: {self.id} Name: {self.name} Output: {self.expected_outputs} Status: {self.status.value} Site: {self.site}"


@dataclass
class GRIDScheduler:
    """placeholder"""

    tasks: list[CNSJob]
    params: dict

    def run(self) -> None:
        """Execute the tasks."""
        queue = {}

        # Convert CNSJobs to GridJobs
        queue = {GridJob(t): False for t in self.tasks}

        log.info(f"Submitting {len(queue)} jobs to the grid...")

        # Submit jobs
        with ThreadPoolExecutor(max_workers=self.params["ncores"]) as executor:
            executor.map(lambda job: job.submit(), queue.keys())

        # Wait for jobs to finish
        total = len(queue)
        complete = False
        while not complete:
            with ThreadPoolExecutor(max_workers=self.params["ncores"]) as executor:
                results = list(
                    # TODO: Refactor this for clarity
                    executor.map(
                        lambda item: (
                            item[0],
                            item[0].update_status()
                            or (
                                log.debug(item[0])
                                if item[0].status != JobStatus.DONE
                                else None
                            )
                            or item[0].status == JobStatus.DONE,
                        ),
                        queue.items(),
                    )
                )
            for job, is_done in results:
                if is_done:
                    queue[job] = True

            # count how many are done
            done = sum(1 for done in queue.values() if done)
            log.info(f"{done}/{total} jobs completed.")
            complete = all(queue.values())

        log.info("All jobs completed.")

        # Retrieve outputs
        with ThreadPoolExecutor(max_workers=self.params["ncores"]) as executor:
            executor.map(lambda job: job.retrieve_output(), queue.keys())

        log.info("All outputs retrieved.")
